{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_user_data(file_path):\n",
    "    # Load data\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # Extract hour from timestamp\n",
    "    data['hour'] = pd.to_datetime(data['timestamp']).dt.hour\n",
    "\n",
    "    # Encode categorical variables\n",
    "    categorical_columns = ['bus_route', 'station_id', 'weather_condition', 'day_of_week', 'card_type', 'boarding_type']\n",
    "    encoders = {}\n",
    "    for col in categorical_columns:\n",
    "        encoders[col] = LabelEncoder()\n",
    "        data[col] = encoders[col].fit_transform(data[col])\n",
    "\n",
    "    # Select features and target\n",
    "    features = ['hour', 'bus_route', 'station_id', 'weather_condition', 'day_of_week', 'holiday_flag', 'card_type', 'boarding_type']\n",
    "    target = 'hourly_boarding_count'\n",
    "    X = data[features]\n",
    "    y = (data[target] > 10).astype(int)  # Binary classification: high (>10) or low (<=10) boarding demand\n",
    "\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, scaler, encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(input_dim, output_dim):\n",
    "    model = models.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_dim=input_dim),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dense(output_dim, activation='tanh')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(input_dim):\n",
    "    model = models.Sequential([\n",
    "        layers.Dense(256, activation='relu', input_dim=input_dim),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dnn(input_dim):\n",
    "    model = models.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_dim=input_dim),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, file_path):\n",
    "    model.save(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(generator, discriminator, gan, X_train, epochs=10000, batch_size=128):\n",
    "    half_batch = batch_size // 2\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Train discriminator\n",
    "        idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
    "        real_data = X_train[idx]\n",
    "        fake_data = generator.predict(np.random.normal(0, 1, (half_batch, X_train.shape[1])))\n",
    "\n",
    "        d_loss_real = discriminator.train_on_batch(real_data, np.ones((half_batch, 1)))\n",
    "        d_loss_fake = discriminator.train_on_batch(fake_data, np.zeros((half_batch, 1)))\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "        # Train generator\n",
    "        noise = np.random.normal(0, 1, (batch_size, X_train.shape[1]))\n",
    "        g_loss = gan.train_on_batch(noise, np.ones((batch_size, 1)))\n",
    "\n",
    "        # Print progress\n",
    "        if epoch % 500 == 0:\n",
    "            print(f\"Epoch {epoch}/{epochs} - D loss: {d_loss[0]:.4f} - G loss: {g_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/10000 - D loss: 0.6815 - G loss: 0.6616\n",
      "Epoch 500/10000 - D loss: 0.2188 - G loss: 2.8646\n",
      "Epoch 1000/10000 - D loss: 0.1561 - G loss: 3.1365\n",
      "Epoch 1500/10000 - D loss: 0.0813 - G loss: 2.8971\n",
      "Epoch 2000/10000 - D loss: 0.0173 - G loss: 4.5717\n",
      "Epoch 2500/10000 - D loss: 0.0050 - G loss: 5.7059\n",
      "Epoch 3000/10000 - D loss: 0.0003 - G loss: 8.1288\n",
      "Epoch 3500/10000 - D loss: 0.0002 - G loss: 8.7415\n",
      "Epoch 4000/10000 - D loss: 0.0001 - G loss: 9.4553\n",
      "Epoch 4500/10000 - D loss: 0.0000 - G loss: 10.6847\n",
      "Epoch 5000/10000 - D loss: 0.0000 - G loss: 10.9265\n",
      "Epoch 5500/10000 - D loss: 0.0000 - G loss: 10.7273\n",
      "Epoch 6000/10000 - D loss: 0.0000 - G loss: 11.9874\n",
      "Epoch 6500/10000 - D loss: 0.0000 - G loss: 9.8976\n",
      "Epoch 7000/10000 - D loss: 0.0000 - G loss: 12.7037\n",
      "Epoch 7500/10000 - D loss: 0.0000 - G loss: 12.0352\n",
      "Epoch 8000/10000 - D loss: 0.0001 - G loss: 11.7769\n",
      "Epoch 8500/10000 - D loss: 0.0001 - G loss: 9.5924\n",
      "Epoch 9000/10000 - D loss: 0.0000 - G loss: 11.5774\n",
      "Epoch 9500/10000 - D loss: 0.0000 - G loss: 12.5540\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/50\n",
      "12800/12800 [==============================] - 1s 75us/sample - loss: 0.4923 - accuracy: 0.8009 - val_loss: 0.1309 - val_accuracy: 0.9822\n",
      "Epoch 2/50\n",
      "12800/12800 [==============================] - 0s 20us/sample - loss: 0.3648 - accuracy: 0.8420 - val_loss: 0.0539 - val_accuracy: 0.9956\n",
      "Epoch 3/50\n",
      "12800/12800 [==============================] - 0s 21us/sample - loss: 0.3222 - accuracy: 0.8609 - val_loss: 0.0241 - val_accuracy: 0.9997\n",
      "Epoch 4/50\n",
      "12800/12800 [==============================] - 0s 19us/sample - loss: 0.2979 - accuracy: 0.8716 - val_loss: 0.0259 - val_accuracy: 0.9975\n",
      "Epoch 5/50\n",
      "12800/12800 [==============================] - 0s 22us/sample - loss: 0.2793 - accuracy: 0.8769 - val_loss: 0.0120 - val_accuracy: 1.0000\n",
      "Epoch 6/50\n",
      "12800/12800 [==============================] - 0s 23us/sample - loss: 0.2633 - accuracy: 0.8839 - val_loss: 0.0105 - val_accuracy: 0.9991\n",
      "Epoch 7/50\n",
      "12800/12800 [==============================] - 0s 21us/sample - loss: 0.2458 - accuracy: 0.8913 - val_loss: 0.0071 - val_accuracy: 1.0000\n",
      "Epoch 8/50\n",
      "12800/12800 [==============================] - 0s 22us/sample - loss: 0.2299 - accuracy: 0.9002 - val_loss: 0.0047 - val_accuracy: 1.0000\n",
      "Epoch 9/50\n",
      "12800/12800 [==============================] - 0s 35us/sample - loss: 0.2139 - accuracy: 0.9092 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
      "Epoch 10/50\n",
      "12800/12800 [==============================] - 1s 41us/sample - loss: 0.1978 - accuracy: 0.9178 - val_loss: 0.0043 - val_accuracy: 0.9991\n",
      "Epoch 11/50\n",
      "12800/12800 [==============================] - 0s 25us/sample - loss: 0.1847 - accuracy: 0.9277 - val_loss: 0.0024 - val_accuracy: 0.9997\n",
      "Epoch 12/50\n",
      "12800/12800 [==============================] - 0s 20us/sample - loss: 0.1708 - accuracy: 0.9360 - val_loss: 0.0061 - val_accuracy: 0.9981\n",
      "Epoch 13/50\n",
      "12800/12800 [==============================] - 0s 19us/sample - loss: 0.1574 - accuracy: 0.9438 - val_loss: 0.0029 - val_accuracy: 0.9991\n",
      "Epoch 14/50\n",
      "12800/12800 [==============================] - 0s 20us/sample - loss: 0.1451 - accuracy: 0.9511 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 15/50\n",
      "12800/12800 [==============================] - 0s 19us/sample - loss: 0.1336 - accuracy: 0.9551 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "12800/12800 [==============================] - 0s 20us/sample - loss: 0.1226 - accuracy: 0.9617 - val_loss: 0.0029 - val_accuracy: 0.9991\n",
      "Epoch 17/50\n",
      "12800/12800 [==============================] - 0s 19us/sample - loss: 0.1153 - accuracy: 0.9658 - val_loss: 0.0025 - val_accuracy: 0.9997\n",
      "Epoch 18/50\n",
      "12800/12800 [==============================] - 0s 21us/sample - loss: 0.1073 - accuracy: 0.9677 - val_loss: 0.0016 - val_accuracy: 0.9997\n",
      "Epoch 19/50\n",
      "12800/12800 [==============================] - 0s 19us/sample - loss: 0.0973 - accuracy: 0.9723 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "12800/12800 [==============================] - 0s 32us/sample - loss: 0.0909 - accuracy: 0.9762 - val_loss: 0.0027 - val_accuracy: 0.9987\n",
      "Epoch 21/50\n",
      "12800/12800 [==============================] - 0s 30us/sample - loss: 0.0840 - accuracy: 0.9802 - val_loss: 0.0041 - val_accuracy: 0.9987\n",
      "Epoch 22/50\n",
      "12800/12800 [==============================] - 0s 22us/sample - loss: 0.0784 - accuracy: 0.9814 - val_loss: 0.0012 - val_accuracy: 0.9997\n",
      "Epoch 23/50\n",
      "12800/12800 [==============================] - 0s 22us/sample - loss: 0.0723 - accuracy: 0.9831 - val_loss: 5.6247e-04 - val_accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "12800/12800 [==============================] - 0s 23us/sample - loss: 0.0676 - accuracy: 0.9850 - val_loss: 0.0045 - val_accuracy: 0.9987\n",
      "Epoch 25/50\n",
      "12800/12800 [==============================] - 0s 22us/sample - loss: 0.0619 - accuracy: 0.9866 - val_loss: 0.0028 - val_accuracy: 0.9991\n",
      "Epoch 26/50\n",
      "12800/12800 [==============================] - 0s 21us/sample - loss: 0.0564 - accuracy: 0.9885 - val_loss: 7.6485e-04 - val_accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "12800/12800 [==============================] - 0s 27us/sample - loss: 0.0537 - accuracy: 0.9897 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "12800/12800 [==============================] - 1s 52us/sample - loss: 0.0484 - accuracy: 0.9918 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "12800/12800 [==============================] - 0s 29us/sample - loss: 0.0466 - accuracy: 0.9923 - val_loss: 6.9140e-04 - val_accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "12800/12800 [==============================] - 1s 54us/sample - loss: 0.0421 - accuracy: 0.9929 - val_loss: 8.4865e-04 - val_accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "12800/12800 [==============================] - 0s 31us/sample - loss: 0.0399 - accuracy: 0.9938 - val_loss: 8.8658e-04 - val_accuracy: 0.9997\n",
      "Epoch 32/50\n",
      "12800/12800 [==============================] - 0s 23us/sample - loss: 0.0352 - accuracy: 0.9957 - val_loss: 8.8898e-04 - val_accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "12800/12800 [==============================] - 0s 21us/sample - loss: 0.0326 - accuracy: 0.9965 - val_loss: 4.9932e-04 - val_accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "12800/12800 [==============================] - 0s 20us/sample - loss: 0.0315 - accuracy: 0.9961 - val_loss: 0.0034 - val_accuracy: 0.9987\n",
      "Epoch 35/50\n",
      "12800/12800 [==============================] - 0s 29us/sample - loss: 0.0293 - accuracy: 0.9960 - val_loss: 3.4592e-04 - val_accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "12800/12800 [==============================] - 0s 21us/sample - loss: 0.0269 - accuracy: 0.9972 - val_loss: 3.6878e-04 - val_accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "12800/12800 [==============================] - 0s 21us/sample - loss: 0.0251 - accuracy: 0.9974 - val_loss: 4.9338e-04 - val_accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "12800/12800 [==============================] - 0s 22us/sample - loss: 0.0241 - accuracy: 0.9973 - val_loss: 2.8183e-04 - val_accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "12800/12800 [==============================] - 0s 21us/sample - loss: 0.0213 - accuracy: 0.9985 - val_loss: 2.6930e-04 - val_accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "12800/12800 [==============================] - 0s 22us/sample - loss: 0.0218 - accuracy: 0.9976 - val_loss: 5.9592e-04 - val_accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "12800/12800 [==============================] - 1s 54us/sample - loss: 0.0191 - accuracy: 0.9984 - val_loss: 6.4024e-04 - val_accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "12800/12800 [==============================] - 0s 21us/sample - loss: 0.0179 - accuracy: 0.9985 - val_loss: 2.9781e-04 - val_accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "12800/12800 [==============================] - 0s 29us/sample - loss: 0.0169 - accuracy: 0.9984 - val_loss: 4.8607e-04 - val_accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "12800/12800 [==============================] - 0s 21us/sample - loss: 0.0161 - accuracy: 0.9982 - val_loss: 3.1835e-04 - val_accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "12800/12800 [==============================] - 0s 26us/sample - loss: 0.0140 - accuracy: 0.9991 - val_loss: 2.0388e-04 - val_accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "12800/12800 [==============================] - 0s 28us/sample - loss: 0.0158 - accuracy: 0.9980 - val_loss: 7.1846e-04 - val_accuracy: 0.9997\n",
      "Epoch 47/50\n",
      "12800/12800 [==============================] - 0s 23us/sample - loss: 0.0137 - accuracy: 0.9985 - val_loss: 2.9503e-04 - val_accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "12800/12800 [==============================] - 0s 28us/sample - loss: 0.0128 - accuracy: 0.9986 - val_loss: 1.3767e-04 - val_accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "12800/12800 [==============================] - 0s 22us/sample - loss: 0.0110 - accuracy: 0.9991 - val_loss: 7.0341e-04 - val_accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "12800/12800 [==============================] - 0s 21us/sample - loss: 0.0106 - accuracy: 0.9989 - val_loss: 1.4243e-04 - val_accuracy: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1546\n",
      "           1       1.00      0.98      0.99       454\n",
      "\n",
      "    accuracy                           1.00      2000\n",
      "   macro avg       1.00      0.99      0.99      2000\n",
      "weighted avg       1.00      1.00      1.00      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load and preprocess user-provided dataset\n",
    "    dataset_path = \"expanded_bus_boarding_demand_dataset.csv\"\n",
    "    X_train, X_test, y_train, y_test, scaler, encoders = load_and_preprocess_user_data(dataset_path)\n",
    "\n",
    "    # GAN setup\n",
    "    input_dim = X_train.shape[1]\n",
    "    generator = build_generator(input_dim=input_dim, output_dim=input_dim)\n",
    "    discriminator = build_discriminator(input_dim=input_dim)\n",
    "    discriminator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Combine generator and discriminator\n",
    "    discriminator.trainable = False\n",
    "    gan_input = layers.Input(shape=(input_dim,))\n",
    "    gan_output = discriminator(generator(gan_input))\n",
    "    gan = models.Model(gan_input, gan_output)\n",
    "    gan.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "    # Train GAN\n",
    "    train_gan(generator, discriminator, gan, X_train)\n",
    "\n",
    "    # Generate synthetic data\n",
    "    noise = np.random.normal(0, 1, (X_train.shape[0], input_dim))\n",
    "    synthetic_data = generator.predict(noise)\n",
    "\n",
    "    # Combine real and synthetic data\n",
    "    X_combined = np.vstack((X_train, synthetic_data))\n",
    "    y_combined = np.hstack((y_train, np.ones(synthetic_data.shape[0])))\n",
    "\n",
    "    # Train the DNN model\n",
    "    dnn = build_dnn(input_dim=input_dim)\n",
    "    dnn.fit(X_combined, y_combined, epochs=50, batch_size=128, validation_split=0.2)\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred = (dnn.predict(X_test) > 0.5).astype(int)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Save models\n",
    "    save_model(generator, \"generator_model.h5\")\n",
    "    save_model(discriminator, \"discriminator_model.h5\")\n",
    "    save_model(dnn, \"dnn_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
